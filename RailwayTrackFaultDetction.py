# -*- coding: utf-8 -*-
"""PD3-Railway Track Fault Detection(RFD)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tVFXIvRL-V37_4WI1PzINGFZs2YAVbk8

##Railway Track Fault Detection
#### Team Members: Arvind Boominathan, Supprethaa Shankar, Namratha Jagadeesh

Importing the required libraries
"""

from google.colab import drive
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image
import numpy as np
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense

"""Mounting the Google Drive to import the dataset"""

drive.mount('/content/drive')

"""Setting the paths for training, test and validation"""

train_dir = '/content/drive/MyDrive/Railway Track fault Detection/Railway Track fault Detection Updated/Train'
val_dir   = '/content/drive/MyDrive/Railway Track fault Detection/Railway Track fault Detection Updated/Validation'
test_dir  = '/content/drive/MyDrive/Railway Track fault Detection/Railway Track fault Detection Updated/Test'

train_defective_fnames = os.listdir(train_dir+'/Defective' )
train_nondefective_fnames = os.listdir(train_dir+'/Non defective')

"""Viewing the structure of the dataset (viewing images)"""

# Parameters for our graph; we'll output images in a 10x10 configuration
nrows = 4
ncols = 4

# Index for iterating over images
pic_index = 0

# Set up matplotlib fig, and size it to fit 4x4 pics
fig = plt.gcf()
fig.set_size_inches(ncols * 4, nrows * 4)

pic_index += 8
next_defective_pix = [os.path.join(train_dir+'/Defective', fname)
                for fname in train_defective_fnames[pic_index-8:pic_index]]
next_nondefective_pix = [os.path.join(train_dir+'/Non defective', fname)
                for fname in train_nondefective_fnames[pic_index-8:pic_index]]


for i, img_path in enumerate(next_defective_pix+next_nondefective_pix):
  # Set up subplot; subplot indices start at 1
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off') # Don't show axes (or gridlines)

  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

"""Exploaratory Data Analysis"""

import os
import matplotlib.pyplot as plt

# Function to count images in each class and plot a pie chart
def plot_class_distribution_pie(folder, title):
    defective_folder = os.path.join(folder, 'Defective')
    non_defective_folder = os.path.join(folder, 'Non defective')

    defective_count = len(os.listdir(defective_folder))
    non_defective_count = len(os.listdir(non_defective_folder))

    class_counts = [defective_count, non_defective_count]
    class_labels = ['Defective', 'Non defective']

    plt.figure(figsize=(8, 8))
    plt.pie(class_counts, labels=class_labels, autopct='%1.1f%%', startangle=90, colors=['red', 'green'])
    plt.title(title)
    plt.show()

# Specify the path to your dataset
dataset_path = '/content/drive/MyDrive/Railway Track fault Detection/Railway Track fault Detection Updated'

# Plot class distribution for the Training set
plot_class_distribution_pie(os.path.join(dataset_path, 'Train'), 'Class Distribution in Training Set')

"""As, observed from the above Pie-graph, there is no class-imbalance between Defective and Non defective images used to train the model. Hence, this can be considered as a balanced dataset

Importing images from the folder to resize it and perform PCA on the images - Feature Engineering
"""

def load_images_from_folder(folder, target_size=(200, 200)):
    images = []
    labels = []
    for subfolder in os.listdir(folder):
        subfolder_path = os.path.join(folder, subfolder)
        if os.path.isdir(subfolder_path):
            for filename in os.listdir(subfolder_path):
                img_path = os.path.join(subfolder_path, filename)

                try:
                    # Open image using PIL and resize
                    img = Image.open(img_path).convert("L")  # Convert to grayscale
                    img_resized = img.resize(target_size)
                    img_array = np.array(img_resized)

                    # Flatten and normalize each image
                    img_flat = img_array.flatten().astype(float) / 255.0
                    images.append(img_flat)
                    labels.append(subfolder)
                except Exception as e:
                    print(f"Error processing {img_path}: {e}")

    return np.array(images), np.array(labels)

"""Performing PCA on the images (Dimensionality Reduction)

1. Loading images
"""

X, y = load_images_from_folder(train_dir)

"""  2. Setting 50 components for reduction"""

# Apply PCA
n_components = 50

"""  3. Creating the PCA for 50 components"""

pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X)

"""  4. Splitting the dataset into training and testing sets as 80% training and 20% validation"""

X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size=0.2, random_state=42)

"""  5. Encoding the labels"""

label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)

"""  6. Plotting the explained variance on the number of components"""

explained_variance = pca.explained_variance_ratio_
cumulative_explained_variance = np.cumsum(explained_variance)

plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='--')
plt.title('Cumulative Explained Variance')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

"""**Implementation of selected classification algorithm**(Convolutional Neural Networks)

>1. Building the CNN model and training
A simple neural network architecture for binary classification (Defective/Non-Defective) with one hidden layer having 128 neurons and a ReLU activation function, and an output layer with 1 neuron and a sigmoid activation function.
Feature Extraction is implicitly performed by the CNN model (detection of edges, corners etc)
"""

model = Sequential()
model.add(Dense(128, input_dim=n_components, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

""">2. Compiling the model"""

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

""">3. Training the CNN model"""

# Train the model
history=model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_val, y_val_encoded))

""">4. Evaluating the model on the validation set"""

# Evaluate the model on the validation set
loss, accuracy = model.evaluate(X_val, y_val_encoded)
print(f'Validation Set Accuracy: {accuracy}')

# Visualize the training and validation accuracy and loss
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(train_acc) + 1)

""">4. Plotting the Training and Validation accuracy"""

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')
plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(epochs, train_loss, 'bo-', label='Training Loss')
plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()

Sfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split

# Split the dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Assuming y_val contains true labels and y_pred_val contains predicted labels for the validation set
y_true = y_val  # Replace with your true validation labels
y_pred = model.predict(X_val)  # Assuming 'model' is your trained model

# Convert the predicted labels if needed (for example, from one-hot to class labels)
# y_pred_classes = np.argmax(y_pred, axis=1)

# Calculate Confusion Matrix
conf_matrix = confusion_matrix(y_true, y_pred_labels)
print("Confusion Matrix:")
print(conf_matrix)

# Calculate Accuracy
accuracy = accuracy_score(y_true, y_pred_labels)
print(f"Accuracy: {accuracy}")

# Calculate Precision, Recall, and F1 Score
precision = precision_score(y_true, y_pred_labels, average='weighted')
recall = recall_score(y_true, y_pred_labels, average='weighted')
f1 = f1_score(y_true, y_pred_labels, average='weighted')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

# Generate Classification Report
class_report = classification_report(y_true, y_pred_labels)
print("Classification Report:")
print(class_report)